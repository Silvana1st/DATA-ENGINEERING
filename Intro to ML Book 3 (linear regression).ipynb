{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./intro_images/HDSbanner.jpg\" width=\"100%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"float:right;\">\n",
    "    <tr>\n",
    "        <td>                      \n",
    "            <div style=\"text-align: right\"><a href=\"https://alandavies.netlify.com\" target=\"_blank\">Dr Alan Davies</a></div>\n",
    "            <div style=\"text-align: right\">Senior Lecturer health data science</div>\n",
    "            <div style=\"text-align: right\">University of Manchester</div>\n",
    "         </td>\n",
    "         <td>\n",
    "             <img src=\"https://github.com/i3hsInnovation/resources/blob/efa61022d0b8893200dad308f6590e694291f8c7/images/alan.PNG?raw=true\" width=\"30%\" />\n",
    "         </td>\n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About this Notebook\n",
    "This notebook introduces the Machine Learning algorithm concept of Linear Regression, used in both traditional statistics and Machine Learning. We will apply these algorithms to some real world data introduced in the notebook Working with Data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Learning Objectives:</b> \n",
    "<br/> At the end of this notebook you will be able to:\n",
    "    \n",
    "- Investigate key features of Linear Regression in statistics and Machine Learning \n",
    "\n",
    "- Explore some essential Machine Learning Python libraries to implement Machine Learning algorithms\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "<b>Table of contents</b><br/>\n",
    "\n",
    "1.0 [Regression](#regression)\n",
    "\n",
    "2.0 [Correlation](#correlation)\n",
    "\n",
    "3.0 [Performing the Regression](#performingregression)\n",
    "\n",
    "4.0 [Your Turn](#yourturn)\n",
    "\n",
    "5.0 [Lasso and Ridge Regression](#lassoridgeregression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is a method used in statistics and Machine Learning to measure the linear relationship between a <code>dependent variable</code> (response) and an <code>independent variable(s)</code> (predictor). The word <code>linear</code> means arranged in a <code>line</code>. The simplest use of linear regression is to do this for one dependent variable and one or more independent variables. Regression has been around for quite a long time and has been attributed to Francis Galton in the later half of the 1800s in a paper called <i>Galton, F. (1886) Regression Towards Mediocrity in Hereditary Stature. Anthropological Miscellanea. pp246-263</i>. The term regression was used in the paper to describe regression towards the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply put, regression will show how much one variable changes as another changes. Typically the variable we are trying to predict (the dependent variable) is referred to by $y$ and the predictor or explanatory variable (independent variable) is referred to by $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>Note:</strong>\n",
    "    To use Linear Regression there must be a linear relationship between the data used. The dependent variable should <strong>not</strong> be ordinal or nominal (there are other forms of regression to deal with variables of these types).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses of regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regression to describe relationships between dependent and independent variables. We can also use it to make predictions. Once we have trained a regression model we can use it on new data to make predictions. For example we might make a regression model to see the rate an ice-cream melts based on the temperature. We could input a temperature and see the resulting predicted melt rate. Linear regression requires us to have a dependent variable that is <code>continuous</code>. These are variables on a scale (e.g. height, weight, temperature). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may remember seeing the equation for a straight line (or some variation): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = mx + c $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $m$ is the gradient (steepness) of the line. $c$ is where the line crosses the y-axis, known as the y-intercept. Let's have a go implementing this in Python. For now don't worry about trying to understand what each line of code does, as this is just for illustrative purposes. First we will import the libraries we need. Libraries contain pre-written functions for carrying out common tasks, in this case plotting graphs and numerical operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates and positions a graph. The main lines of interest here are <code>x = np.linspace(-10,10,100)</code> which sets the range of the graph from -10 to 10. (start, stop and number). Essentially it returns evenly spaced numbers in a specified range. The next is the line <code>plt.plot(x, 3*x+1, '-r', label='y=3x+1')</code>. This adds the line to the graph. The formula $y=3x+1$ is represented in code as <code>3&ast;x+1</code>, the <code>-r</code> is the colour (i.e. r=red) and the <code>label</code> is the text applied to the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "plt.plot(x, 3*x+1, '-r', label='y=3x+1')\n",
    "plt.legend(loc='upper right')\n",
    "plt.plot()\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 1:</b>\n",
    "<br> \n",
    "    Change the code in the cell below to plot a line $y = 2x - 2$<br>\n",
    "    Don't forget to also update the <code>label</code> property.<br>\n",
    "    <strong>HINT:</strong> You only need to make changes to this line: <code>plt.plot(x, 3*x+1, '-r', label='y=3x+1')</code><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['bottom'].set_position('center')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "plt.plot(x, 2*x-2, '-g', label='y=2x-2')\n",
    "plt.legend(loc='upper right')\n",
    "plt.plot()\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient shows the change in y relative to x:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Gradient} = \\frac{\\text{change in y}}{\\text{change in x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for linear regression is very similar to the straight line equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y=\\beta_0 + \\beta x_i+ \\epsilon_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case $y$ is our dependent variable (what we want to predict), $x$ is the explanatory variable. $\\beta_0$ is the y-intercept. We also have this extra term $\\epsilon_i$ which refers to the error in the model. This is the <code>residual error</code> and is the difference between the actual and predicted values. Linear regression assumes an approximate linear relationship between our $x$ and $y$ variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y \\approx \\beta_0 + \\beta_1 x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong>\n",
    "    Essentially the predicted outcome is the result of some model plus some error. There is always some error as the model is only a representation of reality.\n",
    "    $$\\text{Outcome}_i = (model) + \\text{error}_i $$\n",
    "    The model can be a simple representation such as the <code>mean</code>, or a much more complex model. In the case of linear regression, we assume we can model the data with a line of 'best fit' that goes through or as close to as many data points as possible.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality we don't know what the values are for the <code>intercept</code> and the <code>slope</code>. We represent these model <code>parameters</code> (or <code>coefficients</code>) with $\\beta_0$ and $\\beta_1$ pronounced 'beta'. A <code>regression line</code> or <code>line of best fit</code> is a line that fits the data the best (i.e. goes through as many points as possible). If the model fits the line exactly there is a perfect linear relationship between the variables. In reality this is unlikely and there is usually some error (called <code>residuals</code>). The dashed lines in the image below show these residuals. The greater the difference between the data point and the line, the greater the error. We can determine the overall error by summing these residuals. To prevent the positive and negative values canceling each other out, we square the values (to remove the sign). So we use the <code>sum of squared values</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ml_files/lr.png\" width=\"50%\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line with a positive gradient shows a positive relationship between the variables in question whereas as a negative gradient shows a negative relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong>\n",
    "    The best regression line can be determined in several ways. One common way is the <code>Ordinary Least Squares (OLS)</code> method. This selects the line with the lowest squared differences. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to top](#top)\n",
    "\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "<a id=\"correlation\"></a>\n",
    "\n",
    "## 2.0 Correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand regression, it is helpful to first understand correlation. If you are familiar with this concept you can skip over this background. Correlation quantifies the strength of a relationship between 2 variables. Imagine the relationship between temperature and speed at which ice cream melts. The higher the temperature, the faster the ice cream melts. The reverse is also true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>Note:</strong>\n",
    "    Correlation does not imply causation. So just because two variables are correlated does not mean that one causes an effect/phenomenon. There may be other unknown factors. Consider that a large proportion of people die in bed. Does this mean beds are killing people? There could be other factors like disease or old age that are the real underlying cause. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one variable deviates from its mean then we would expect the other variable to vary in a similar way (if there is some relationship between the two). If this is the case, they are said to <code>covary</code>. <code>Variance</code> is the average amount data varies from the mean. To get around the fact that the units of measure used to compare variance can be different, they can be standardized in what is referred to as the <code>standard deviation</code> which measures how much individual members of a group are different from the mean of the group. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong>\n",
    "    Variance: ($\\bar{x}$ is the mean of the sample and $N$ is the number of observations, $x_i$ are the individual values and $\\bar{x}$ is the mean of the sample)<br>\n",
    "    $$s^2 = \\frac{\\sum (x_i - \\bar{x}^2)}{N-1} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have standardized the variables using the standard deviation, we can calculate the standardized covariance between two variables. This is otherwise known as the <code>correlation coefficient</code>. This was created by Karl Pearson an English mathematician and bio-statistician and is more formally known as the <code>Pearson product-moment correlation coefficient</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong>\n",
    "    There are other correlation coefficients for dealing with non parametric data such as Spearman's rank correlation coefficient (Spearman's Rho) and where there are multiple ties, Kendall's rank correlation coefficient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider an example. In this case we have some data on the amount of hours that students studied for an exam and the grade they received. We might intuitively guess that students who study for longer would get a higher grade. Let's add some data to 2 lists. The first is the number of hours studied which we will store in a variable called <code>hours_studied</code> and the next is the grade percentage and will be stored in <code>grade_pc</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_studied = [2, 3, 3, 6, 7, 8, 10, 11, 4, 6, 7, 3, 7, 12]\n",
    "grade_pc = [40, 55, 53, 65, 67, 70, 71, 80, 46, 62, 66, 50, 66, 91]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then display a scatter graph to visually see if it looks like there might be a relationship between these 2 variables. Here we add the lists as parameters to the <code>scatter()</code> function as well as the labels for the x and y axis. At first glance it looks like there is some positive relationship between the 2 variables. We can quantify this by applying a correlation test to the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(hours_studied, grade_pc);\n",
    "plt.xlabel('Hours studied');\n",
    "plt.ylabel('Grade (%)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import the <code>pearsonr</code> function form the <code>scipy.stats</code> package. Next we pass in our two lists and store the result in a variable called <code>cor</code> for correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "cor = pearsonr(hours_studied, grade_pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we output the values we get what Python calls a <code>tuple</code>, an ordered unchangeable list. The first value is the Pearson’s correlation coefficient and the second is the 2-tailed p-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> A p-value is used to accept or reject a null hypothesis. A null hypothesis states there is no statistically significant difference between populations and that any differences are the result of error (e.g. sampling or in the experimental design). The lower the p-value the more confident we can be at rejecting the null hypothesis and that the differences seen are the result of some inherent differences between the groups observed. In fields like psychology and human computer interaction a p-value of $<$ 0.05 is considered sufficient to reject the null hypotheses, whereas in some areas of medicine it is rejected at $<$ 0.001. There is a lot of controversy about the p-value, for example <a href=\"https://www.nature.com/articles/d41586-019-00857-9\" target=\"_blank\">Scientists rise up against statistical significance</a>.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access individual values of the tuple we can refer to the required item using square brackets. In Python this starts at 0, so the correlation coefficient is at location 0 and the p-value at location 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation coefficient:\", cor[0])\n",
    "print(\"p-value: %f\" % cor[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we changed the format of the p-value as this was in scientific notation to something more readable. We could report this as a statistically significant ($\\alpha < 0.001$) strong positive correlation $r = .95, p < .001$. All correlation values should range between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It if often useful to check the explanatory variables that you intend to use for regression against one another using correlation. If any two are highly correlated, it is good practice to remove one of them. This can help to prevent the model from over-fitting. One way to do this quickly is to use a correlation matrix. Let's take a look at some real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 2:</b>\n",
    "<br> \n",
    "Take a look at the dataset which we will use. This is the <code>Life Expectancy Data</code> <a href=\"https://www.kaggle.com/kumarajarshi/life-expectancy-who/data\" target=\"_blank\">dataset</a> from Kaggle. The data is from the The Global Health Observatory (GHO) data repository under World Health Organization (WHO). Click on the dataset link above and read the description of the data fields (columns) in the section called <code>About this file</code> so that you understand what sort of data is contained in the dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import the <code>pandas</code> library and load the dataset. This is currently stored as a Comma Separated Values (CSV) file. We load the data and store it in a variable called <code>data</code>. This allows us to store the data in a <code>data frame</code> object which lets us view and manipulate the data as a table. Next we output the first 10 records using the <code>head()</code> function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./ml_files/Life Expectancy Data.csv\"\n",
    "data = pd.read_csv(path)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a correlation matrix to view the correlation between variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = data.corr(method=\"pearson\")\n",
    "corr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this can be quite hard to read, we could represent it as a heatmap diagram instead. Negative correlations are displayed in red and positive in the blue/green colour. The stronger the colour, the stronger the correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(corr_data, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200), square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason some of the columns in the dataframe have spaces (called whitespace) at the beginning and/or end of the column names. That means that instead of accessing the column <code>Life expectancy</code> like this <code>data['Life expectancy']</code>, one would need to do it like this <code>data['Life expectancy ']</code>. Note the extra space at the end of the word 'expectancy'. You can see the problem if we loop over all the column names and print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> Little problems and issues like this are what takes most of the time in data science. The preprocessing of the data to get it into the required format is by far the most laborious (and often overlooked) task.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately there are more Python functions that can help us with this. The first line removes whitespace from the start of the column name <code>lstrip()</code> where the <code>l</code> is for left. The second line removes whitespace from the end of the column name. <code>r</code> for right. You can see when we print them out a second time that the issue is resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.lstrip()\n",
    "data.columns = data.columns.str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest case of Linear Regression is to apply it to just two variables. For this example we will use <code>Life expectancy</code> as the variable we want to predict and <code>Schooling</code> as the predictor. Life expectancy is measured in years and Schooling also is measured in years of schooling. If you recall there was a high correlation between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 3:</b>\n",
    "<br> \n",
    "Have a look at the correlation table above.<br>\n",
    "    1. What was the correlation coefficient for <code>Life expectancy</code> and <code>Schooling</code>?<br>\n",
    "    2. Is this a positive or negative correlation?<br> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "$r = 0.751975$. This suggests a strong positive correlation between the two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to help us visualize this, let's create a quick scatter plot to compare the 2 variables visually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['Schooling'], data['Life expectancy']);\n",
    "plt.ylabel('Life expectancy (years)');\n",
    "plt.xlabel('Years of schooling');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that people with more years of schooling generally live longer. This is an example of where we need to be cautious about causation. There are clearly other factors at work here. For example higher education may lead to better paid employment which can provide access to better healthcare. We can also see a bunch of people with zero years of schooling who live into their mid to late 70's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we should do is check our two variables to see how many missing values there are in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> Missing data can cause a variety of issues including:<br>\n",
    "    <ul>\n",
    "        <li>Bias in parameter estimation</li>\n",
    "        <li>Reduction of sample representativeness</li>\n",
    "        <li>Reduction of statistical power</li>\n",
    "    </ul>\n",
    "    All of this can lead to invalid conclusions. If you want to find out more about this, have a look at this article <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/#:~:text=Second%2C%20the%20lost%20data%20can,can%20lead%20to%20invalid%20conclusions.\" target=\"_blank\">The prevention and handling of the missing data</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two useful Python functions for this. We can use <code>isna()</code> to check for missing values and <code>sum()</code> to add them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of rows:\", len(data['Life expectancy']))\n",
    "print(\"Missing values in 'Life expectancy':\", data['Life expectancy'].isna().sum())\n",
    "print(\"Missing values in 'Schooling':\", data['Schooling'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not many missing values in Life expectancy but in the Schooling variable we have around 6% of missing values. To resolve this we will replace these missing values with the mean of the data for that column. We can then check again for missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with the mean\n",
    "data['Life expectancy'] = data['Life expectancy'].fillna(data['Life expectancy'].mean())\n",
    "data['Schooling'] = data['Schooling'].fillna(data['Schooling'].mean())\n",
    "\n",
    "# Check again for missing values\n",
    "print(\"Missing values in 'Life expectancy':\", data['Life expectancy'].isna().sum())\n",
    "print(\"Missing values in 'Schooling':\", data['Schooling'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> We could do the same to an entire dataframe like so: <code>df = df.fillna(df.mean())</code> where <code>df</code> is the name of your dataframe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to top](#top)\n",
    "\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "<a id=\"performingregression\"></a>\n",
    "\n",
    "# 3.0 Performing the Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to extract the required columns from the dataframe and ensure they are in the correct format. The <code>reshape()</code> function changes the 1D array into a 2D array which is the required format for the Linear Regressor we will use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['Schooling'].values.reshape(-1,1)\n",
    "y = data['Life expectancy'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to split the data into a <code>training</code> and <code>test</code> set. We typically use around 20% of the data to test the model and the remaining 80% to carry out the training. We import the required function <code>train_test_split</code> that can do this for us. Next we tell it the <code>test_size=0.2</code>. This is where we specify that we want to use 20% of the data for testing. The results of this function will be stored in the variables <code>X_train, X_test, y_train, y_test</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ml_files/model.png\" width=\"60%\" align=\"center\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can create a new Linear Regressor. We will call this <code>lr</code> for Linear Regressor. We also need to first import this from the <code>sklearn</code> library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained the model we need to test it. This is what our test variables are used for. So we can try to make some predictions. By convention these are stored in a variable <code>y_pred</code> (y prediction). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put these values into a dataframe and output the first 10 so we can compare the actual vs predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual vaues': y_test.flatten(), 'Predicted values': y_pred.flatten()})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also produce a plot of our predicted data vs our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test)\n",
    "plt.plot(X_test, y_pred, color='red') \n",
    "plt.ylabel('y_test');\n",
    "plt.xlabel('X_test');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> We can also view information about the slope and intercept like so:<br>\n",
    "    <code>print(lr.coef_)</code> and <code>print(lr.intercept_)</code>. Where <code>lr</code> is the name of your regressor. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions using new data using the <code>predict()</code> function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of model performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several metrics that can be used to measure the model's performance. These commonly include:<br>\n",
    "<ul>\n",
    "    <li><strong>Residual Sum of Squares (RSS):</strong> The sum of errors (residuals) squared</li>\n",
    "    <li><strong>Mean Squared Error (MSE):</strong> The average of prediction errors (residuals)</li>\n",
    "    <li><strong>Root Mean Squared Error (RMSE):</strong> The standard deviation of prediction errors (residuals)</li>\n",
    "    <li><strong>Mean Absolute Error (MAE):</strong> Measures errors between observed and predicted values</li>\n",
    "    <li><strong>$R^2$:</strong> Proportion of variance in dependent variable explained by explanatory variable(s) (0-1)</li>\n",
    "    <li><strong>Adjusted $R^2$:</strong> A modified $R^2$ adjusted for the number of parameters in a model</li>\n",
    "    <li><strong>F ratio:</strong> Ratio of model fit to data; significant p-value indicates good fit</li>\n",
    "</ul>\n",
    "Several of these metrics are closely related:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Residual Sum of Squares (RSS)\n",
    "$$\\text{RSS} = \\displaystyle\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "The Mean Squared Error (MSE) is the average of the RSS\n",
    "$$\\text{MSE} = \\frac{1}{N}\\displaystyle\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "The Root Mean Squared Error (RMSE) is just the square root of the MSE\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{N}\\displaystyle\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again Python has plenty of support for this with <code>sklearn.metrics</code>. Including the <code>mean_squared_error</code>, <code>mean_absolute_error</code> and <code>r2_score </code> functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred)) \n",
    "print(\"Root Mean Squared Error (RMSE):\", np.sqrt(mean_squared_error(y_test, y_pred))) \n",
    "print(\"Mean Absolute Error (MAE):\",  mean_absolute_error(y_test, y_pred))\n",
    "print(\"The R^2 score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the MSE, the higher the number the larger the error. This implies a greater difference between the observed and predicted values. Values are square so that positive and negative values do not cancel each other out. Unlike a p-value, the value of MSE has no optimal value but as said the lower the better. The same is true for the RMSE. $R^2$ provides a proportion so 1 (100%) would be the best value. This is also useful because other values are expressed in units of y. The $R^2$ is thus independent of the y-scale. This gives us the proportion of variance that is explained by the explanatory variable(s). We can also get standardized beta coefficients showing how the dependent variable changes in relation to each unit of the independent variable. Another way of indicating the goodness of fit of the model is the F-ratio with an associated p-value, which indicates whether your linear regression model provides a better fit to the data than a model that contains no independent variables. Unfortunately the F-ratio is not provided in the <code>sklearn.metrics</code> package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> If you did want to obtain an F-statistic and p-value you could use the <code>statsmodels.api</code> package to run an Ordinary Least Squares (OLS), which would obtain these (and more) statistical properties.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple regression is useful because it allows you to assess the relationship between two variables while controlling for potential confounders. For example you can look at the relationship between income and life expectancy while controlling for other factors such as age by including age in the model as a predictor. Before you have a go with a new dataset. Let's first quickly run a multiple linear regression with the previous dataset. For analysis you may want to subset your data to examine a specific subset of interest. For example we can look at just the UK rather than all the countries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK = data['Country'] == 'United Kingdom of Great Britain and Northern Ireland'\n",
    "UK_data = data[UK]\n",
    "UK_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore this data using visualization. This is covered in more depth in the notebook Working with Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(UK_data['Year'], UK_data['Life expectancy']);\n",
    "plt.ylabel('Life expectancy (years)');\n",
    "plt.xlabel('Year');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variable encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>Note:</strong>\n",
    "    We can't use categorical variables as they are in a regression model. We first need to recode them. This applies to many Machine Learning algorithms. Categories must first be converted into number format. For more information on variable encoding see <a href=\"https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\" target=\"_blank\">Categorical encoding using Label-Encoding and One-Hot-Encoder</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is convert the categorical data to integer (numeric) values. To do this we can use the <code>factorize()</code> function. For example the column <code>Country</code> contains the names of countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = data['Country']\n",
    "country_encoded, country_categories = country_data.factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoded countries:\")\n",
    "print(country_encoded)\n",
    "print(\"Country categories:\")\n",
    "print(country_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this is that some algorithms will assume that there is some significance in the values used. For example 1 and 10 are less similar than 1 and 2. We can overcome this by using <code>one hot encoding</code>. This works by creating a sparse matrix. Where the value is found a 1 (hot) is used, otherwise a 0 (cold) is used. For example, let's say we had some fruit like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> A sparse matrix is a matrix where most of the values are 0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>id</td>\n",
    "        <td>Fruit</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>Apple</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>Orange</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>Pear</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>Banana</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used one-hot encoding it would look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>id</td>\n",
    "        <td>Apple</td>\n",
    "        <td>Orange</td>\n",
    "        <td>Pear</td>\n",
    "        <td>Banana</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "        <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td>\n",
    "        <td>1</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the <code>OneHotEncoder</code> class from the <code>sklearn.preprocessing</code> package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ec = OneHotEncoder(categories='auto')\n",
    "country_encoded_hot = ec.fit_transform(country_encoded.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_encoded_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<strong>Note:</strong> In the future this can be done in one go with \n",
    "    <code>from sklearn.preprocessing import CategoricalEncoder</code> when it is implemented.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a way to do this easily using the pandas library with the <code>get_dummies()</code> function that uses one-hot encoding by default. Here we can specify a column and a prefix (i.e. Country_country name). We can then display the first 5 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = pd.get_dummies(data['Country'], prefix='Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, we can do this, add it to the existing data and replace the original column of categorical values all in one go as seen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_country = pd.concat([data, pd.get_dummies(data['Country'], prefix='Country', dummy_na=True)], axis=1).drop(['Country'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 4:</b>\n",
    "<br> \n",
    "    1. Using the code in the line above the table where we assign a value to the variable <code>data_encoded_country</code>, repeat the one-hot encoding for the <code>Status</code> column and store the result in a variable called <code>all_data_encoded</code>.<br>\n",
    "    2. Output the first 10 records using the <code>head()</code> function.<br>\n",
    "    <strong>HINT:</strong> Remember to use the variable <code>data_encoded_country</code> instead of <code>data</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "all_data_encoded = pd.concat([data_encoded_country, pd.get_dummies(data_encoded_country['Status'], prefix='Status', dummy_na=True)], axis=1).drop(['Status'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "all_data_encoded.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's move the column we want to predict (<code>Life expectancy</code>) to the end of the dataset so we can prepare it for the regression analysis. First we use the <code>pop()</code> function to remove the data and store it in a variable (<code>life_expectancy</code>) then we can add it to the end of the dataframe. This rearranging just makes it easier to select the features and labels later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expecancy = all_data_encoded.pop('Life expectancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_encoded['Life expectancy'] = life_expecancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be able to train the model if the data contains any NA values. To overcome this we can replace those missing values with the mean. This time we do this for the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>Note:</strong> There are lots of different methods for replacing missing values, this can include imputing the mean, median or using chained equations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_encoded.isnull().any()\n",
    "all_data_encoded = all_data_encoded.fillna(all_data_encoded.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into features <code>X</code> and labels <code>y</code> by selecting all columns except the last for our features and the last column for the labels (dependent variable). Remember we moved the <code>Life expectancy</code> column to the end. Next we split the dataset into training and testing sets (80% training and 20% testing) as before. Then we create an instance of the regressor (called <code>lr</code>), train the model, make some predictions and output some metrics to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_data_encoded.iloc[:, 0:-1].values\n",
    "y = all_data_encoded.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred)) \n",
    "print(\"Root Mean Squared Error (RMSE):\", np.sqrt(mean_squared_error(y_test, y_pred))) \n",
    "print(\"Mean Absolute Error (MAE):\",  mean_absolute_error(y_test, y_pred))\n",
    "print(\"The R^2 score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line makes it so we can display the whole table in a scrollable window in the notebook environment. Then we display the coefficient for each feature. What this shows is what effect a unit increase or decrease on a feature has. Specifically it shows how the mean of the dependent variable is predicted to change in response to a single unit change of an independent variable while holding all the other variables constant. A positive sign means as the value increases, the mean of the dependent variable increases. If the sign is negative the mean of the dependent variable decreases.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = list(all_data_encoded.columns)\n",
    "col_names.pop()\n",
    "coeffs = pd.DataFrame(lr.coef_, col_names, columns=['Coefficient'])  \n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's round up the values to remove the scientific notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values represent the units of measure in the underlying variable. So for example if <code>HIV/AIDS</code> increases by a unit which is measured in <code>Deaths per 1 000 live births HIV/AIDS (0-4 years)</code> then <code>Life expectancy</code> decreases by 0.3 years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>Note:</strong>\n",
    "   In a real world case, you would not want to include all the parameters in your model. You would remove those that are irrelevant to your analysis, especially as many datasets are underpowered to include large numbers of predictors (particularly when the $R^2$ is small). You can also use dimensionality reduction methods to reduce the number of features. There are also types of regression like Lasso regression (discussed later) that produce sparse models through variable selection. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>Note:</strong> We can optimize the regression line in Machine Learning using a technique called <code>gradient descent</code>. In fact this technique can be used with many loss (a.k.a error, cost) functions in Machine Learning to improve our models. If you want to know about gradient descent, watch this YouTube <a href=\"https://www.youtube.com/watch?v=sDv4f4s2SB8\" target=\"_blank\">video</a> for an overview.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to check various <code>assumptions</code> hold true before you can run a linear regression. These include:<br>\n",
    "<ul>\n",
    "    <li><strong>Linearity:</strong> There must exist a linear relationship between the dependent and independent variables. The dependent variable must be a linear function of the independent variables.\n",
    "    <li><strong>Independence of errors:</strong> The residuals should be independent of one another. \n",
    "    <li><strong>Homoscedasticity :</strong> (a.k.a. homogeneity of variance). The data points should be similarly scattered around the line. Similar distances from the line. (the opposite of this is heteroscedasticity).\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to top](#top)\n",
    "\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "<a id=\"yourturn\"></a>\n",
    "\n",
    "# 4.0 Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we will use a dataset about medical insurance. We will load the dataset and store it in a variable called <code>insurance_data</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./ml_files/Insurance.csv\"\n",
    "insurance_data = pd.read_csv(path)\n",
    "insurance_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Task 5:</b> \n",
    "<br> \n",
    "1. Read about the dataset <a href=\"https://www.kaggle.com/mirichoi0218/insurance?select=insurance.csv\" target=\"_blank\">here</a>.<br>\n",
    "2. Using the techniques described above, carry out a linear regression on the dataset using <code>charges</code> as the dependent variable.<br> \n",
    "3. Evaluate the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to top](#top)\n",
    "\n",
    "\n",
    "----------\n",
    "\n",
    "\n",
    "<a id=\"lassoridgeregression\"></a>\n",
    "\n",
    "# 5.0 Lasso and Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other forms of regression available depending on the data you have. For example predicting a binary (2 categories) outcome we could use <code>Logistic Regression</code>. For data with with a high degree of <code>multicollinearity</code> we could use <code>Ridge Regression</code>. A non-exhaustive list of different types of regression can be seen here:<br>\n",
    "<ul>\n",
    "    <li>Linear Regression</li>\n",
    "    <li>Polynomial Regression</li>\n",
    "    <li>Logistic Regression</li>\n",
    "    <li>Ridge Regression</li>\n",
    "    <li>Lasso Regression</li>\n",
    "    <li>Elastic Net Regression</li>\n",
    "    <li>Principal Components Regression</li>\n",
    "    <li>Support Vector Regression</li>\n",
    "    <li>Ordinal Regression</li>\n",
    "    <li>Poisson Regression</li>\n",
    "    <li>Negative Binomial Regression</li>\n",
    "    <li>...</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take a closer look at two popular types of regression (<code>Lasso Regression</code>, <code>Ridge Regression</code>) that are used for <code>regularization</code> in data science. Both can be used to reduce the number of features in the model and prevent over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>Note:</strong> We want our Machine Learning algorithms to be <strong>generalizable</strong> so we can use them with different data. If the model is <strong>over-fitting</strong> then the model fits the training data too well, including the noise in the data. In contrast if there is <strong>under-fitting</strong> in the model it doesn't model the training data well. It is often necessary to fine tune the model to try and find a point between these two extremes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO (Least Absolute Shrinkage and Selection Operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression adds what is called a <code>regularization term</code> to the <code>cost function</code>. Lasso can completely eliminate the weights of the least important features in a model by setting them to zero. This helps with feature selection by removing or reducing the weights of the less important features in a model (creating a sparse model). Lasso uses the $L_1$ norm which is the sum of the coefficients absolute values (see $L_1$ and $L_2$ regularization below for more details). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>Note:</strong> A cost function is usually a distance or difference between predicted and observed values. It is also referred to as <strong>error</strong> or <strong>loss</strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous encoded dataset <code>all_data_encoded</code> we will again split the data in feature <code>X</code> and labels <code>y</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_data_encoded.iloc[:, 0:-1].values\n",
    "y = all_data_encoded.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can output the number of features (-1 to exclude the label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features:\", len(all_data_encoded.columns)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the data into training and test sets and import the required class (<code>Lasso</code>), create an instance and then train the model. Hopefully you can see that this is exactly the same as the linear regression we performed earlier but with <code>Lasso</code> instead of <code>LinearRegression</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>Note:</strong> The error message seen above denotes that the model has failed to converge. With iterative algorithms, the output should get closer to some optimal value. This can be a sign that the model did not fit the data very well. We may need to remove additional features before running the Lasso or increase the number of iterations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can sum the coefficients that are not 0. We can see we have gone from 216 features in the model down to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of features:\", np.sum(lasso.coef_ != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression uses $L_2$ regularization and aims to keep the model weights as small as possible. It does this by adding a regularization term to the cost function:\n",
    "$$\\alpha \\displaystyle\\sum_{i=1}^{n} \\theta_i^2$$\n",
    "So unlike the Lasso regression that can have 0 value coefficients, Ridge regression has near-zero coefficients. We can alter the alpha ($\\alpha$) parameter to improve the model. Making alpha larger will move the coefficients closer to zero. This can help improve the generalizability of the model but at the cost of a decrease in performance on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>Note:</strong> When we create our instance of the ridge class we can pass in a different alpha ($\\alpha$) value like so <code>ridge = Ridge(alpha=0.2)</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a quick example using the same data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some of the metrics from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error (MSE):\", mean_squared_error(y_test, y_pred)) \n",
    "print(\"Root Mean Squared Error (RMSE):\", np.sqrt(mean_squared_error(y_test, y_pred))) \n",
    "print(\"Mean Absolute Error (MAE):\",  mean_absolute_error(y_test, y_pred))\n",
    "print(\"The R^2 score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>Note:</strong> Regularization is used to prevent over-fitting by penalizing the coefficients of the model to improve how generalizable the model is. They also increase robustness against colinearity of ordinary least squares regression. $L_1$ is more robust than $L_2$. If you want to read more about $L_1$ and $L_2$ norms, read this information on <a href=\"https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms\" target-\"_blank\">Kaggle</a>.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook details\n",
    "<br>\n",
    "<i>Notebook created by <strong>Dr. Alan Davies</strong> with, <strong>Frances Hooley</strong> and <strong>Dr. Jon Parkinson</strong>\n",
    "\n",
    "Publish date: April 2020<br>\n",
    "Review date: September 2021</i>\n",
    "\n",
    "Please give your feedback using the button below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"typeform-share button\" href=\"https://hub11.typeform.com/to/tLz68lSn\" data-mode=\"popup\" style=\"display:inline-block;text-decoration:none;background-color:#3A7685;color:white;cursor:pointer;font-family:Helvetica,Arial,sans-serif;font-size:18px;line-height:45px;text-align:center;margin:0;height:45px;padding:0px 30px;border-radius:22px;max-width:100%;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;font-weight:bold;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;\" target=\"_blank\">Rate this notebook </a> <script> (function() { var qs,js,q,s,d=document, gi=d.getElementById, ce=d.createElement, gt=d.getElementsByTagName, id=\"typef_orm_share\", b=\"https://embed.typeform.com/\"; if(!gi.call(d,id)){ js=ce.call(d,\"script\"); js.id=id; js.src=b+\"embed.js\"; q=gt.call(d,\"script\")[0]; q.parentNode.insertBefore(js,q) } })() </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
